{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3946043c-98d5-4bb4-a754-7e1737f851e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3e36311d4541b5a0757e2bbfacf79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0cda3-6f96-42c8-9b38-fa61f110845e",
   "metadata": {},
   "source": [
    "**Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5be1147-e1a7-4758-97c3-f508afffe2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe8d4234aaa4c3cb43e2a4cc5374a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JAHID\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807679555d4042dc85f0565f2033034d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88db0b458e5b4896a27242e044fc9996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff01b30a50f4f1bb51e7967876232f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba3a89eef8846dd9fe6b929610ae306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55083ee866f54bbca0c58ad51efd6275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9466cd07501e4c609fb269de30304bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08a788ea87246029091f44d7b971a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4061e8bc-94cc-44fc-a10e-358ee7e41162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello how are you?\\n\\nI'm trying to figure out the best way to handle a potential conflict with a colleague.\\n\\nWe've been\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline = pipeline(task=\"text-generation\",model=model,tokenizer=tokenizer)\n",
    "\n",
    "generation_pipeline(\"Hello how are you?\",max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652ef5a-d549-4a48-ae91-02f97edd4f0c",
   "metadata": {},
   "source": [
    "**Batch Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d418b5a2-a016-42c0-aebf-4a21a1e5945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Hello what are you?\\n\\nIt is difficult to know what you are.'}],\n",
       " [{'generated_text': 'The capital of Bangladesh is Dhaka. It is a bustling city with a rich'}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline([\n",
    "    \"Hello what are you?\",\n",
    "    \"The capital of Bangladesh is\"\n",
    "],max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "866ab4a0-6a4c-4e75-bf4a-50c72f66a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755181fa-4a54-49b1-b67d-073bf2bac3c2",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb4a2734-226e-4e56-a766-0607f634df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "input_prompt = [\n",
    "    \"Hello how are you doing tell me?\",\n",
    "    \"The capital of Bangladesh is\"\n",
    "]\n",
    "tokenized = tokenizer(input_prompt,padding=True,return_tensors=\"pt\")\n",
    "print(tokenized[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71aad523-b5a1-4929-82e7-229684faafcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,   9259,   1217,    659,    611,   3490,   3442,    786, 236881],\n",
       "        [     1,      1,      1,      2,    818,   5279,    529,  33752,    563]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6742ffac-a936-4298-9b1f-4eab061dd02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>Hello how are you doing tell me?',\n",
       " '<eos><eos><eos><bos>The capital of Bangladesh is']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97b755-2cdd-4b10-9081-a248b5c75bb5",
   "metadata": {},
   "source": [
    "**Chat Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7a7d95c-4e1b-43c2-984d-4cb31e0f92fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    105,   2364,    107,   3048,    659,  11045,  12498,  22083,\n",
      "            108,  10936,    531,   2748,    528,  15313,  19180,    528, 111425,\n",
      "            106,    107,    105,   4368,    107,  51462,    657,    106,    107,\n",
      "            105,   4368,    107]])\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are helpful AI Assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Where to study in Machine Learning in Dhaka\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Study at\"\n",
    "    }\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt=True,\n",
    "    contine_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e255d2d5-c989-41a8-8a61-e7c193667d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(tokenized,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aedbbf29-4c41-4ea1-a1a1-9ef98247337d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,    105,   2364,    107,   3048,    659,  11045,  12498,  22083,\n",
       "            108,  10936,    531,   2748,    528,  15313,  19180,    528, 111425,\n",
       "            106,    107,    105,   4368,    107,  51462,    657,    106,    107,\n",
       "            105,   4368,    107,  19058, 236764,   1531, 236789, 236751,   2541,\n",
       "           1679,   1298,    611,    740,   2748,  15313,  19180,    528, 111425,\n",
       "         236761,   1030, 236858, 236751,    496]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28c02a67-101e-4fc0-b511-68d2787b8066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are helpful AI Assistant\n",
      "\n",
      "Where to study in Machine Learning in Dhaka<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Study at<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, let's break down where you can study Machine Learning in Dhaka. There are several excellent options, ranging from formal institutions to online platforms. Hereâ€™s a breakdown, categorized by type and with estimated costs (which can fluctuate):\n",
      "\n",
      "**1. Formal Universities & Colleges (Highly Recommended)**\n",
      "\n",
      "* **SRJ University:** (Highly Recommended - Often considered the top choice)\n",
      "    * **Program:** Master of Science in Data Science and Machine Learning (MSDSML)\n",
      "    *\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ab51b-ef55-4e26-b0ee-f63ecff0ea4b",
   "metadata": {},
   "source": [
    "**Lora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e791cd48-bc93-4f2f-a934-50e27427d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,981,888 || all params: 1,002,867,840 || trainable%: 0.2973\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj','v_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model,lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ffc002b-5df9-470c-acbf-e581242dcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prompt = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Where to study in Machine Learning in Dhaka?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Study at\"\n",
    "    }\n",
    "]\n",
    "target_response = \"Interactive Care\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8b087dd-ff29-42bf-9aaf-86a6e94a1863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Where to study in Machine Learning in Dhaka?\n",
      "model\n",
      "Study at Machine Learning in Dhaka is a bit tricky because it\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(training_prompt,continue_final_message=True,return_tensors=\"pt\")\n",
    "test_out = model.generate(test_tokenized,max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(test_out,skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b4817bf-6ef4-40dd-bff1-01a745d483c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_response):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt,continue_final_message=True,tokenize=False)\n",
    "    full_response_text = [\n",
    "        (chat_template + \" \" + target_response + tokenizer.eos_token)\n",
    "        for chat_template, target_response in zip(chat_templates,target_response)\n",
    "    ]\n",
    "    input_ids_tokenized = tokenizer(full_response_text,return_tensors=\"pt\",add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    labels_tokenized = tokenizer([\" \"+response+tokenizer.eos_token for response in target_response],\n",
    "                               add_special_tokens=False,return_tensors=\"pt\",padding=\"max_length\",max_length=input_ids_tokenized.shape[1])[\"input_ids\"]\n",
    "\n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_ids,labels_tokenized,-100)\n",
    "    labels_tokenized_fixed[:,-1] = tokenizer.pad_token_id\n",
    "\n",
    "    input_ids_tokenized_left_shifted= input_ids_tokenized[:,:-1]\n",
    "    input_ids_tokenized_right_shifted= input_ids_tokenized[:,1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"attention_mask\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": input_ids_tokenized_left_shifted,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30f9730e-ec21-41d8-a36d-13a175098039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1,logits.size(-1)),labels.view(-1))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92659882-4d41-4249-87c9-b3c9f21e07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.633334755897522\n",
      "loss:  1.2560653686523438\n",
      "loss:  0.7621409893035889\n",
      "loss:  0.5239904522895813\n",
      "loss:  0.4461246728897095\n",
      "loss:  0.456743061542511\n",
      "loss:  0.4348416328430176\n",
      "loss:  0.4238668382167816\n",
      "loss:  0.4177795350551605\n",
      "loss:  0.41063806414604187\n",
      "loss:  0.40056726336479187\n",
      "loss:  0.39028969407081604\n",
      "loss:  0.38052016496658325\n",
      "loss:  0.3710635304450989\n",
      "loss:  0.35946932435035706\n",
      "loss:  0.3355048596858978\n",
      "loss:  0.3184804916381836\n",
      "loss:  0.3028225302696228\n",
      "loss:  0.2771447002887726\n",
      "loss:  0.24802570044994354\n"
     ]
    }
   ],
   "source": [
    "data = generate_input_output_pair(prompt=[training_prompt],target_response=[target_response])\n",
    "data[\"input_ids\"] = data[\"input_ids\"]\n",
    "data[\"labels\"] = data[\"labels\"]\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr=1e-3,weight_decay=0.01)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = model(input_ids=data[\"input_ids\"])\n",
    "    loss = calculate_loss(out.logits,data[\"labels\"]).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e201d7b7-7876-4071-81e3-d020a990d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Where to study in Machine Learning in Dhaka?\n",
      "model\n",
      "Study at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(training_prompt,continue_final_message=True,return_tensors=\"pt\")\n",
    "test_out = model.generate(test_tokenized,max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(test_out,skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
